<!DOCTYPE html>
<html>
<head>
    <title>Harley Ao - Project 4</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
        }
        img {
            border-radius: 8px;
        }
        nav a {
            color: rgba(17, 106, 87, 0.5);
        }
    </style>
</head>
<body>
    <nav style="display: flex; gap: 18px; align-items: center; padding: 10px 0; margin-bottom: 28px; border-bottom: 2px solid rgba(17, 106, 87, 0.5);">
        <a href="/index.html" style="text-decoration: none; font-weight: bold; padding: 6px 14px;">Home</a>
        <a href="/0/index.html" style="text-decoration: none; font-weight: bold; padding: 6px 14px;">Project 0</a>
        <a href="/1/index.html" style="text-decoration: none; font-weight: bold; padding: 6px 14px;">Project 1</a>
        <a href="/2/index.html" style="text-decoration: none; font-weight: bold; padding: 6px 14px;">Project 2</a>
        <a href="/3/index.html" style="text-decoration: none; font-weight: bold; padding: 6px 14px;">Project 3</a>
        <a href="/4/index.html" style="text-decoration: none; font-weight: bold; padding: 6px 14px;">Project 4</a>
        <a href="/5/index.html" style="text-decoration: none; font-weight: bold; padding: 6px 14px;">Project 5</a>
        <a href="https://cal-cs180.github.io/fa25/hw/proj4/index.html" style="margin-left: auto; text-decoration: none; font-weight: bold; padding: 6px 14px;">Spec</a>
    </nav>
    <h1>Project 4: Neural Radiance Field</h1>
    <h2> Part 0: Calibrating Your Camera and Capturing a 3D Scan</h2>
    <p> 
        After calibrating my camera, I captured a 3D scan of my Pop Mart figure. 
        Below are the estimates of the camera's extrinsic parameters.
    </p>
    <div style="display: flex; gap: 20px; justify-content: center;">
        <div>
            <h3 style="text-align: center;"> Example Image </h3>
            <a href="/4/media/0_training.JPG" target="_blank">
                <img src="/4/media/0_training.JPG" alt="Example Image" width="300">
            </a>
        </div>
    </div>
    <div style="display: flex; gap: 20px; justify-content: center;">
        <div>
            <h3 style="text-align: center;"> Horizontal View </h3>
            <a href="/4/media/0.3_horizontalview.png" target="_blank">
                <img src="/4/media/0.3_horizontalview.png" alt="Horizontal View" width="600">
            </a>
        </div>
        <div>
            <h3 style="text-align: center;"> Vertical View </h3>
            <a href="/4/media/0.3_verticalview.png" target="_blank">
                <img src="/4/media/0.3_verticalview.png" alt="Vertical View" width="600">
            </a>
         </div>
    </div>

    <h2> Part 1: Fit a Neural Field to a 2D Image </h2>
    <h3> Architecture:</h3>
    <ul style="margin-left: 20px;">
        <li>Layers: 4</li>
        <li>Width: 256 (tested 32 and 256)</li>
        <li>Positional Encoding: 2D positional encoding with max frequency L (tested with L=0 and L=4)</li>
        <li>Input Dimension: 2 + 4L</li>
        <li>Output: 3 Channels (RGB), using Sigmoid to output values in [0,1]</li>
    </ul>
    <h3> Training Hyperparameters:</h3>
    <ul style="margin-left: 20px;">
        <li>Optimizer: Adam</li>
        <li>Learning Rate: 0.01</li>
        <li>Loss Function : Mean Squared Error (MSE)</li>
        <li>Batch Size: 10,000 pixels per iteration</li>
        <li>Iterations : 2,000</li>
    </ul>
    <p>
        We train a coordinate-based MLP with 4 hidden layers, ReLU activations, and a Sigmoid RGB output. 
        The input is 2D pixel coordinates encoded with sinusoidal positional encoding of dimension 2+4L. 
        We optimize with Adam with a learning rate of 0.01, MSE loss, batch size 10,000 pixel samples, for 2,000 iterations per setting.
    </p>
    <h3>
        Fox Image Training Progression:
    </h3>
    <p>
        Below are the results of my training progression on the provided test image with different hyperparameters.
    </p>
    <div style="display: flex; gap: 20px; justify-content: center;">
        <div>
            <h3 style="text-align: center;"> Original Image </h3>
            <a href="/4/media/fox.jpg" target="_blank">
                <img src="/4/media/fox.jpg" alt="Original Image" width="600">
            </a>
        </div>
    </div>
    <div style="display: flex; gap: 20px; justify-content: center;">
        <div>
            <h3 style="text-align: center;"> Iterations </h3>
            <a href="/4/media/1_fox_training.png" target="_blank">
                <img src="/4/media/1_fox_training.png" alt="Iterations" width="1200">
            </a>
        </div>
    </div>
    <div style="display: flex; gap: 20px; justify-content: center;">
        <div>
            <h3 style="text-align: center;">  Final Results </h3>
            <a href="/4/media/1_fox_hyperparameter.png" target="_blank">
                <img src="/4/media/1_fox_hyperparameter.png" alt="Final Results" width="600">
            </a>
        </div>
        <div>
            <h3 style="text-align: center;"> PSNR Curve </h3>
            <a href="/4/media/1_fox_psnr.png" target="_blank">
                <img src="/4/media/1_fox_psnr.png" alt="PSNR Curve" width="500">
            </a>
        </div>
    </div>
    <h3>
        Laptop Image Training Progression:
    </h3>
    <p>
        Now, I use an image of my laptop to further test my model with different hyperparameters.
        My results from the test image and this laptop image illustrate the importance of positional encoding in capturing high-frequency details.
        The PSNR curve for the laptop image grew slower compared to the fox image, corroborating with the fact that the laptop image is more complex and requires more iterations to converge.
    </p>
    <div style="display: flex; gap: 20px; justify-content: center;">
        <div>
            <h3 style="text-align: center;"> Original Image </h3>
            <a href="/4/media/laptop.jpg" target="_blank">
                <img src="/4/media/laptop.jpg" alt="Original Image" width="600">
            </a>
        </div>
    </div>
    <div style="display: flex; gap: 20px; justify-content: center;">
        <div>
            <h3 style="text-align: center;"> Iterations </h3>
            <a href="/4/media/1_laptop_training.png" target="_blank">
                <img src="/4/media/1_laptop_training.png" alt="Iterations" width="1200">
            </a>
        </div>
    </div>
    <div style="display: flex; gap: 20px; justify-content: center;">
        <div>
            <h3 style="text-align: center;">  Final Results </h3>
            <a href="/4/media/1_laptop_hyperparameter.png" target="_blank">
                <img src="/4/media/1_laptop_hyperparameter.png" alt="Final Results" width="600">
            </a>
        </div>
        <div>
            <h3 style="text-align: center;"> PSNR Curve </h3>
            <a href="/4/media/1_laptop_psnr.png" target="_blank">
                <img src="/4/media/1_laptop_psnr.png" alt="PSNR Curve" width="500">
            </a>
        </div>
    </div>
    <h2> Part 2: Fit a Neural Radiance Field from Multi-view Images </h2>
    <h3 style="text-align: center;"> 2.1: Create Rays from Cameras</h3>
    <p>
        For 2.1, I first implemented a transform(T, x) function to convert normalized pixel coordinates in an image to 3D coordinates.
        To convert 2D pixel coordinates back to 3D camera coordinates, I implemented pixel_to_camera(K, uv, s), which uses the camera extrinsic matrix to calculate the transformations.
        Lastly, I defined pixel_to_ray(K, c2w, uv), which combines the previous two functions to compute the rays for each pixel in the image by calculating the origin and direction of the rays in world coordinates.  
    </p>
    <h2> Part 2: Fit a Neural Radiance Field from Multi-view Images </h2>
    <h3 style="text-align: center;"> 2.2: Sampling</h3>
    <p>
        For 2.2, I expanded on my functions from 2.1 to randomly sample a fixed number of rays per training iteration.
        My process for this function was to load all images and their corresponding poses (c2w) and intrinsics (K).
        Then, I computed the rays for each pixel in every image using pixel_to_ray(K, c2w, uv).
        Finally, I randomly selected a fixed number of rays from the entire set of rays across all images to return a batch containing ray origins, directions, and RGB values.
    </p>
    <h3 style="text-align: center;"> 2.3: Putting the Dataloading All Together</h3>
    <p>
        Here are the visualizations of the work I did from the previous 2 parts using the lego dataset.
    </p>
    <div style="display: flex; gap: 20px; justify-content: center;">
        <div>
            <h3 style="text-align: center;">  All Cameras and Rays </h3>
            <a href="/4/media/2.3_image1.png" target="_blank">
                <img src="/4/media/2.3_image1.png" alt="All Cameras and Rays" width="600">
            </a>
        </div>
        <div>
            <h3 style="text-align: center;"> Rays from One Camera </h3>
            <a href="/4/media/2.3_image2.png" target="_blank">
                <img src="/4/media/2.3_image2.png" alt="Rays from One Camera" width="600">
            </a>
        </div>
    </div>
</body>
</html>
